{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/developer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/developer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mod=load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f6a498df3c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports for pre-processing of images and other utlity libraries\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix , classification_report , accuracy_score\n",
    "\n",
    "#Import the deep learning libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History\n",
    "from keras.applications import vgg16, vgg19, inception_v3, xception, resnet, inception_v3, inception_resnet_v2, mobilenet, mobilenet, nasnet, mobilenet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_epoch_error(x, val_error, train_error, colors=['b']):\n",
    "    \"\"\"\n",
    "    This function is used to plot the loss vs epoch for the\n",
    "    trained models using the History callback.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.grid()\n",
    "    plt.plot(x, val_error, 'b', label=\"Validation Loss\")\n",
    "    plt.plot(x, train_error, 'r', label=\"Train Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    \"\"\"\n",
    "    Given the ground truth and the predicted labels,\n",
    "    this function is used to plot the confusion matrix,\n",
    "    recall matrix and the precision matrix using seaborn\n",
    "    heatmaps.\n",
    "    \"\"\"\n",
    "    C = confusion_matrix(test_y, predict_y) #Confusion Matrix\n",
    "    A =(((C.T)/(C.sum(axis=1))).T) #Recall Matrix\n",
    "    B =(C/C.sum(axis=0)) #Precision Matrix\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    labels = [0,1]\n",
    "    \n",
    "    #Representing Confusion Matrix(C) in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    #Representing Precision Matrix(B) in heatmap format\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    #Representing Recall Matrix(B) in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-5-6a479c944b0f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6a479c944b0f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    filepath\u001b[0m\n\u001b[0m            \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "filepath=\"\"\"weights\\\"\"\"\n",
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 --no-check-certificate\n",
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 --no-check-certificate\n",
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5 --no-check-certificate\n",
    "!wget https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large-no-top.h5 --no-check-certificate\n",
    "!wget https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 --no-check-certificate\n",
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels.h5 --no-check-certificate              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"C:\\\\Users\\\\Hp\\\\Documents\\\\e-Crash-master\\\\weights\\\\\"\n",
    "os.listdir(filepath)\n",
    "\n",
    "base_model = inception_v3.InceptionV3(weights=None, include_top=False)\n",
    "base_model.load_weights(filepath+\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=load_models(\"inceptionv3\")\n",
    "#Adding a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "def load_data():\n",
    "    df_train=pd.read_csv(\"train.csv\")\n",
    "    df_test=pd.read_csv(\"test.csv\")\n",
    "    df_val=pd.read_csv(\"val.csv\")\n",
    "    return df_train, df_test, df_val\n",
    "\n",
    "#Creating the base pre-trained model (VGG16)\n",
    "#https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "def load_models(name):\n",
    "    \"\"\"\n",
    "    Initialize the pre-trained model architecture and load the model weights.\n",
    "    The downloaded weights contains only the convolution base. It does not \n",
    "    contain the top two dense layers. We will have to manually define the top\n",
    "    two dense layers.\n",
    "    \"\"\"\n",
    "    filepath=\"C:\\\\Users\\\\Hp\\\\Documents\\\\e-Crash-master\\\\weights\\\\\"\n",
    "    if(name=='vgg16'):\n",
    "        base_model = vgg16.VGG16(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    elif(name==\"inceptionv3\"):\n",
    "        base_model = inception_v3.InceptionV3(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    elif(name==\"resnet50\"):\n",
    "        base_model = resnet.ResNet50(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    elif(name==\"inception_resnet\"):\n",
    "        base_model = inception_resnet_v2.InceptionResNetV2(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\")        \n",
    "    elif(name==\"nasnet\"):\n",
    "        base_model = nasnet.NASNetLarge(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"NASNet-large-no-top.h5\")   \n",
    "    elif(name==\"xception\"):\n",
    "        base_model = xception.Xception(weights=None, include_top=False)\n",
    "        base_model.load_weights(filepath+\"inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\")      \n",
    "    return base_model\n",
    "\n",
    "#Define custom callbacks\n",
    "def callbacks_list(monitor):\n",
    "    filepath=\"C:\\\\Users\\\\Hp\\\\Documents\\\\e-Crash-master\\\\models\\\\\"\n",
    "    checkpoint = ModelCheckpoint(filepath+\"weights-{epoch:02d}-{val_acc:.2f}.hdf5\", \n",
    "                                 monitor=monitor, \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=False, \n",
    "                                 mode='auto',\n",
    "                                 period=1)\n",
    "\n",
    "    reduce_learning_rate = ReduceLROnPlateau(monitor=monitor, patience=10)\n",
    "    early_stop = EarlyStopping(monitor=monitor, patience=10)\n",
    "    history = History()\n",
    "    list_ = [checkpoint, reduce_learning_rate, early_stop, history]\n",
    "    return list_\n",
    "\n",
    "def train_stage1(model_name, dense_neurons, classes, batch_size, epochs, stage1_lr):\n",
    "    base_model = load_models(model_name)\n",
    "    #Adding a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    #Adding a fully-connected dense layer\n",
    "    x = Dense(dense_neurons, activation='relu', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    #Adding a final dense output final layer\n",
    "    predictions = Dense(classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    #Define the model\n",
    "    model_stg1 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    #Here we will freeze the convolution base and train only the top layers\n",
    "    #We will set all the convolution layers to false, the model should be\n",
    "    #compiled when all the convolution layers are set to false\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #Compiling the model\n",
    "    model_stg1.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #Normalize the images\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "                  \n",
    "    df_train, df_test, df_val = load_data()\n",
    "    \n",
    "    delim='\\\\'\n",
    "    source=\"data_split\"\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=df_train,\n",
    "                                                        directory=os.path.abspath(source+\"{}train\".format(delim)),\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        x_col=\"filenames\",\n",
    "                                                        y_col=\"class_label\",                                                    \n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "    val_generator = val_datagen.flow_from_dataframe(dataframe=df_val,\n",
    "                                                    directory=os.path.abspath(source+\"{}validation\".format(delim)),\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    x_col=\"filenames\",\n",
    "                                                    y_col=\"class_label\",                                                    \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(dataframe=df_test,\n",
    "                                                      directory=os.path.abspath(source+\"{}test\".format(delim)),\n",
    "                                                      target_size=(224, 224),\n",
    "                                                      x_col=\"filenames\",\n",
    "                                                      y_col=\"class_label\",                                                    \n",
    "                                                      batch_size=batch_size,\n",
    "                                                      class_mode='categorical')\n",
    "\n",
    "\n",
    "    nb_train_samples = len(train_generator.classes)\n",
    "    nb_val_samples = len(val_generator.classes)\n",
    "    nb_test_samples = len(test_generator.classes)\n",
    "\n",
    "    model_stg1.fit_generator(train_generator,\n",
    "                        steps_per_epoch=nb_train_samples // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_generator,\n",
    "                        validation_steps=nb_val_samples // batch_size,\n",
    "                        callbacks=callbacks_list())\n",
    "                  \n",
    "    params=[train_generator,\n",
    "            val_generator,\n",
    "            test_generator,\n",
    "            nb_train_samples,\n",
    "            nb_val_samples,\n",
    "            nb_test_samples, \n",
    "            batch_size]\n",
    "    \n",
    "    return model_stg1, params\n",
    "\n",
    "def train_stage2(stage2_lr, model_stg2, model_name, epochs):\n",
    "    #At this point, the top layers are well trained and we can start fine-tuning\n",
    "    #convolutional layers of the pre-trained architecture. We will freeze the bottom \n",
    "    #N layers and train the remaining top layers.\n",
    "\n",
    "    #We will train the top 2 blocks and we will freeze the first N layers\n",
    "    #A very low learning rate is used, so as to not wreck the convolution base with \n",
    "    #massive gradient updates\n",
    "                         \n",
    "    if(model_name=='vgg16'):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True\n",
    "    elif(name==\"inceptionv3\"):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True\n",
    "    elif(name==\"resnet50\"):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True\n",
    "    elif(name==\"inception_resnet\"):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True        \n",
    "    elif(name==\"nasnet\"):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True   \n",
    "    elif(name==\"xception\"):\n",
    "        for layer in model_stg2.layers[:15]:\n",
    "           layer.trainable = False\n",
    "        for layer in model_stg2.layers[15:]:\n",
    "           layer.trainable = True      \n",
    "\n",
    "    #Recompile the model, train the top 2 blocks\n",
    "    model_stg2.compile(optimizer=optimizers.RMSprop(lr=stage2_lr), \n",
    "                       loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    #Re-train the model again (this time fine-tuning the top 2 inception blocks\n",
    "    #alongside the top Dense layers\n",
    "    model_stg2.fit_generator(params[0],\n",
    "                             steps_per_epoch=nb_train_samples // params[6],\n",
    "                             epochs=epochs,\n",
    "                             validation_data=params[1],\n",
    "                             validation_steps=params[4] // params[6],\n",
    "                             callbacks=callbacks_list())\n",
    "    return model_stg2\n",
    "                  \n",
    "def train(model_name, dense_neurons, classes, batch_size, epochs,stage1_lr,stage2_lr):\n",
    "    model_stg1, params = train_stage1(model_name, dense_neurons, classes, batch_size, epochs, stage1_lr)\n",
    "    model_stg2 = train_stage2(stage2_lr, model_stg1, model_name, epochs)\n",
    "    \n",
    "    return model_stg1, model_stg2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9d3bdad981e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inceptionv3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstage1_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstage2_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-b8406db3457a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model_name, dense_neurons, classes, batch_size, epochs, stage1_lr, stage2_lr)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstage1_lr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstage2_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mmodel_stg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_stage1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage1_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mmodel_stg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_stage2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage2_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_stg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-b8406db3457a>\u001b[0m in \u001b[0;36mtrain_stage1\u001b[1;34m(model_name, dense_neurons, classes, batch_size, epochs, stage1_lr)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_stage1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage1_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;31m#Adding a global spatial average pooling layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-b8406db3457a>\u001b[0m in \u001b[0;36mload_models\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"inceptionv3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"resnet50\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1228\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1230\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1231\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[0mfiltered_layer_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_attributes_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_names'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[0moid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0motype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\"inceptionv3\", dense_neurons=128, classes=2, batch_size=5, epochs=2,stage1_lr=0.00001,stage2_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim='\\\\'\n",
    "source=\"data_split\"\n",
    "os.path.abspath(source+\"{}train\".format(delim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=os.path.abspath(source+\"{}\".format(delim))\n",
    "filepath=\"'C:\\\\Users\\\\Hp\\\\Documents\\\\e-Crash-master\\\\models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom callbacks\n",
    "def callbacks_list():\n",
    "    filepath=os.path.abspath(source+\"{}\".format(delim))\n",
    "    checkpoint = ModelCheckpoint(\"weights-{epoch:02d}-{val_acc:.2f}.hdf5\", \n",
    "                                 monitor='val_acc', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=False, \n",
    "                                 mode='max',\n",
    "                                 period=1)\n",
    "\n",
    "    reduce_learning_rate = ReduceLROnPlateau(monitor='val_acc', patience=10)\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=10)\n",
    "    history = History()\n",
    "    list_ = [checkpoint, reduce_learning_rate, early_stop, history]\n",
    "    return list_\n",
    "\n",
    "list_=callbacks_list()\n",
    "list_\n",
    "    \n",
    "\n",
    "delim='\\\\'\n",
    "source=\"data_split\"\n",
    "os.path.abspath(source+\"{}train\".format(delim))\n",
    "\n",
    "#Training the model on sample dataset\n",
    "#Normalize the images\n",
    "\n",
    "batch_size = 5\n",
    "epochs = 2\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Normalize the images\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=df_train,\n",
    "                                                    directory=os.path.abspath(source+\"{}train\".format(delim)),\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    x_col=\"filenames\",\n",
    "                                                    y_col=\"class_label\",                                                    \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe=df_val,\n",
    "                                                    directory=os.path.abspath(source+\"{}validation\".format(delim)),\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    x_col=\"filenames\",\n",
    "                                                    y_col=\"class_label\",                                                    \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=df_test,\n",
    "                                                    directory=os.path.abspath(source+\"{}test\".format(delim)),\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    x_col=\"filenames\",\n",
    "                                                    y_col=\"class_label\",                                                    \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "\n",
    "nb_train_samples = len(train_generator.classes)\n",
    "nb_val_samples = len(val_generator.classes)\n",
    "nb_test_samples = len(test_generator.classes)\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size,\n",
    "                    callbacks=callbacks_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best weights and save the final model\n",
    "model.load_weights(\"vgg19_weights.hdf5\")\n",
    "model.save(\"vgg19_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#At this point, the top layers are well trained and we can start fine-tuning\n",
    "#convolutional layers of the pre-trained architecture. We will freeze the bottom \n",
    "#N layers and train the remaining top layers.\n",
    "\n",
    "#Visualize the layer names and indices to see how many layers do we need to train\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "#We will train the top 2 blocks and we will freeze the first N layers\n",
    "#A very low learning rate is used, so as to not wreck the convolution base with \n",
    "#massive gradient updates\n",
    "for layer in model.layers[:15]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[15:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "#Recompile the model, train the top 2 blocks\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=RMSprop(lr=0.000001), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Re-train the model again (this time fine-tuning the top 2 inception blocks\n",
    "#alongside the top Dense layers\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=10,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=nb_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.history\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(0,len(history.history['val_loss'])))\n",
    "\n",
    "#Plot train vs test loss\n",
    "val_error = history.history['val_loss'] \n",
    "train_error = history.history['loss'] \n",
    "plt_epoch_error(x, val_error, train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Handwritten','Printed']\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
